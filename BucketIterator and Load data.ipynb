{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BucketIterator functionality to group examples(sequences) of similar lengths into batches. This allows us to provide the most optimal batches when training models with text data.\n",
    "\n",
    "Provides a lot of gain for recurrent models (RNN, GRU, LSTM) and transformers models (bert, roBerta, gpt2, xlnet, etc.) where padding will be minimal.\n",
    "\n",
    "The BucketIterator is similar in applying Dataloader to a PyTorch Dataset.\n",
    "\n",
    "PyTorchText can handle splits! No need to write same line of code again for train and validation split.\n",
    "\n",
    ".tsv file (Tab-Separated Values).\n",
    ".csv comma-separated values\n",
    "\n",
    "An Iterator will sample a batch of sentences, each will (usually) have different lengths, say: 32, 25, 10 tokens in each sentence. It will then look at the longest sentence within the batch, here 32 tokens, and then all sentences shorter than that will be padded to that length, 32 tokens. This will be done by appending a pad_token, which is <pad> by default, to the end of the sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bucketiterator: don't fix length in data.field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _*_ coding: utf-8 _*_\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "from torchtext.vocab import Vectors, GloVe\n",
    "\n",
    "def load_dataset(test_sen=None):\n",
    "\n",
    "    \"\"\"\n",
    "    tokenizer : Breaks sentences into a list of words. If sequential=False, no tokenization is applied\n",
    "    Field : A class that stores information about the way of preprocessing\n",
    "    fix_length : An important property of TorchText is that we can let the input to be variable length, and TorchText will\n",
    "                 dynamically pad each sequence to the longest sequence in that \"batch\". But here we are using fi_length which\n",
    "                 will pad each sequence to have a fix length of 200.\n",
    "                 \n",
    "    build_vocab : It will first make a vocabulary or dictionary mapping all the unique words present in the train_data to an\n",
    "                  idx and then after it will use GloVe word embedding to map the index to the corresponding word embedding.\n",
    "                  \n",
    "    vocab.vectors : This returns a torch tensor of shape (vocab_size x embedding_dim) containing the pre-trained word embeddings.\n",
    "    BucketIterator : Defines an iterator that batches examples of similar lengths together \n",
    "                to minimize the amount of padding needed.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    tokenize = lambda x: x.split()\n",
    "    TEXT = data.Field(sequential=True, tokenize=tokenize, lower=True, \n",
    "                      include_lengths=True, batch_first=True, fix_length=200)\n",
    "    LABEL = data.LabelField(dtype=torch.FloatTensor)\n",
    "    train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\n",
    "    TEXT.build_vocab(train_data, vectors=GloVe(name='6B', dim=300))\n",
    "    LABEL.build_vocab(train_data)\n",
    "\n",
    "    word_embeddings = TEXT.vocab.vectors\n",
    "    print (\"Length of Text Vocabulary: \" + str(len(TEXT.vocab)))\n",
    "    print (\"Vector size of Text Vocabulary: \", TEXT.vocab.vectors.size())\n",
    "    print (\"Label Length: \" + str(len(LABEL.vocab)))\n",
    "\n",
    "    train_data, valid_data = train_data.split() # Further splitting of training_data to create new training_data & validation_data\n",
    "    train_iter, valid_iter, test_iter = data.BucketIterator.splits((\n",
    "        train_data, valid_data, test_data), batch_size=32, sort_key=lambda x: len(x.text), \n",
    "        repeat=False, shuffle=True)\n",
    "\n",
    "    '''Alternatively we can also use the default configurations'''\n",
    "    # train_iter, test_iter = datasets.IMDB.iters(batch_size=32)\n",
    "\n",
    "    vocab_size = len(TEXT.vocab)\n",
    "\n",
    "    return TEXT, vocab_size, word_embeddings, train_iter, valid_iter, test_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Text Vocabulary: 251639\n",
      "Vector size of Text Vocabulary:  torch.Size([251639, 300])\n",
      "Label Length: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cytao/.local/lib/python3.6/site-packages/torchtext/data/iterator.py:48: UserWarning: BucketIterator class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<torchtext.data.field.Field at 0x7fa1bbc87d68>,\n",
       " 251639,\n",
       " tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0466,  0.2132, -0.0074,  ...,  0.0091, -0.2099,  0.0539],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]),\n",
       " <torchtext.data.iterator.BucketIterator at 0x7fa0e97d2fd0>,\n",
       " <torchtext.data.iterator.BucketIterator at 0x7fa0e97d2da0>,\n",
       " <torchtext.data.iterator.BucketIterator at 0x7fa0e97d2e48>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def files_to_tsv(partition_path, save_path='./'):\n",
    "    \"\"\"Parse each file in partition and keep track of sentiments.\n",
    "    Create a list of pairs [tag, text]\n",
    "    Arguments:\n",
    "        partition_path (:obj:`str`):\n",
    "        Partition used: train or test.\n",
    "        save_path (:obj:`str`):\n",
    "            Path where to save the final .tsv file.\n",
    "    Returns:\n",
    "        :obj:`str`: Filename of created .tsv file.\n",
    "    \"\"\"\n",
    "\n",
    "    # List of all examples in format [tag, text].\n",
    "    examples = []\n",
    "\n",
    "    # Print partition.\n",
    "    print(partition_path)\n",
    "\n",
    "    # Loop through each sentiment.\n",
    "    for sentiment in ['pos', 'neg']:\n",
    "\n",
    "        # Find path for sentiment.\n",
    "        sentiment_path = os.path.join(partition_path, sentiment)\n",
    "\n",
    "        # Get all files from path sentiment.\n",
    "        files_names = os.listdir(sentiment_path)\n",
    "\n",
    "        # For each file in path sentiment.\n",
    "        for file_name in tqdm(files_names, desc=f'{sentiment} Files'):\n",
    "\n",
    "            # Get file content.\n",
    "            file_content = io.open(os.path.join(sentiment_path, file_name), mode='r', encoding='utf-8').read()\n",
    "            # Fix any format errors.\n",
    "            file_content = fix_text(file_content)\n",
    "            # Append sentiment and file content.\n",
    "            examples.append([sentiment, file_content])\n",
    "\n",
    "    # Create a TSV file with same format `sentiment  text`.\n",
    "    examples = [\"%s\\t%s\"%(example[0], example[1]) for example in examples]\n",
    "\n",
    "    # Create file name.\n",
    "    tsv_filename = os.path.basename(partition_path) + '_pos_neg_%d.tsv'%len(examples)\n",
    "    # Write to TSV file.\n",
    "    io.open(os.path.join(save_path, tsv_filename), mode='w', encoding='utf-8').write('\\n'.join(examples))\n",
    "\n",
    "    # Return TSV file name.\n",
    "    return tsv_filename\n",
    "  \n",
    "\n",
    "# Path where to save tsv file.\n",
    "data_path = '/content'\n",
    "\n",
    "# Convert train files to tsv file.\n",
    "train_filename = files_to_tsv(partition_path='/content/aclImdb/train', save_path=data_path)\n",
    "\n",
    "# Convert test files to tsv file.\n",
    "test_filename = files_to_tsv(partition_path='/content/aclImdb/test', save_path=data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text tokenizer function - dummy tokenizer to return same text.\n",
    "# Here you will use your own tokenizer.\n",
    "text_tokenizer = lambda x : x\n",
    "\n",
    "# Label tokenizer - dummy label encoder that returns same label.\n",
    "# Here you will add your own label encoder.\n",
    "label_tokenizer = lambda x: x\n",
    "\n",
    "# Data field for text column - invoke tokenizer.\n",
    "TEXT = torchtext.data.Field(sequential=True, tokenize=text_tokenizer, lower=False)\n",
    "\n",
    "# Data field for labels - invoke tokenize label encoder.\n",
    "LABEL = torchtext.data.Field(sequential=True, tokenize=label_tokenizer, use_vocab=False)\n",
    "\n",
    "# Create data fields as tuples of description variable and data field.\n",
    "datafields = [(\"label\", LABEL),\n",
    "              (\"text\", TEXT)]\n",
    "\n",
    "\n",
    "# Group similar length text sequences together in batches.\n",
    "torchtext_train_dataloader, torchtext_valid_dataloader = torchtext.data.BucketIterator.splits(\n",
    "    \n",
    "                              # Datasets for iterator to draw data from\n",
    "                              (train_dataset, valid_dataset),\n",
    "\n",
    "                              # Tuple of train and validation batch sizes.\n",
    "                              batch_sizes=(train_batch_size, valid_batch_size),\n",
    "\n",
    "                              # Device to load batches on.\n",
    "                              device=device, \n",
    "\n",
    "                              # Function to use for sorting examples.\n",
    "                              sort_key=lambda x: len(x.text),\n",
    "\n",
    "\n",
    "                              # Repeat the iterator for multiple epochs.\n",
    "                              repeat=True, \n",
    "\n",
    "                              # Sort all examples in data using `sort_key`.\n",
    "                              sort=False, \n",
    "\n",
    "                              # Shuffle data on each epoch run.\n",
    "                              shuffle=True,\n",
    "\n",
    "                              # Use `sort_key` to sort examples in each batch.\n",
    "                              sort_within_batch=True,\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of number of epochs.\n",
    "epochs = 1\n",
    "\n",
    "# Example of loop through each epoch.\n",
    "for epoch in range(epochs):\n",
    "\n",
    "  # Create batches - needs to be called before each loop.\n",
    "  torchtext_train_dataloader.create_batches()\n",
    "\n",
    "  # Loop through BucketIterator.\n",
    "  for sample_id, batch in enumerate(torchtext_train_dataloader.batches):\n",
    "    # Put all example.text of batch in single array.\n",
    "    batch_text = [example.text for example in batch]\n",
    "\n",
    "    print('Batch examples lengths: %s'.ljust(20) % str([len(text) for text in batch_text]))\n",
    "\n",
    "    # Let's break early, you get the idea.\n",
    "    if sample_id == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
