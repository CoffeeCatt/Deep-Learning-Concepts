{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian kernel\n",
    "For ${x_i}$ and ${y_j}$: \n",
    "$$\\exp\\left(-\\frac{\\sum_{i,j}(x_i-y_j)^2}{bandwidth}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MMD: distances between mean embeddings of features.\n",
    "\n",
    "Distributions $P$ and $Q$ over a set $X$.\n",
    "The MMD is defined by a feature map $\\phi:X→H$, $H$: reproducing kernel Hilbert space.\n",
    "$$\\text{MMD}(P,Q)=\\|\\mathbb{E}_{X \\sim P}[\\phi(X)]−\\mathbb{E}_{Y \\sim Q}[\\phi(Y)]\\|$$\n",
    "\n",
    "Simplest: $X=H=\\mathbb{R}^d$ and $\\phi(x)=x$.\n",
    "\n",
    "$$\\text{MMD}(P,Q)=\\|\\mathbb{E}_{X\\sim P}[X]−\\mathbb{E}_{Y\\sim Q}[Y]\\|_{\\mathbb{R}^𝑑}\n",
    "=\\|\\mu_P-\\mu_Q\\|_{\\mathbb{R}^𝑑},$$\n",
    "Matching distributions like this will match their means.\n",
    "\n",
    "Stronger:\n",
    "$X=\\mathbb{R}, \\phi(x)=(x, x^2)$,\n",
    "$$\\text{MMD}(P,Q)=\\sqrt{(\\mathbb{E}X-\\mathbb{E}Y)^2 +(\\mathbb{E}X^2-\\mathbb{E}Y^2)^2}$$\n",
    "\n",
    "Much stronger: $\\phi$ maps to a general RKHS --> apply the kernel trick to compute the MMD, including the Gaussian kernel, lead to the MMD being zero if and only the distributions are identical.\n",
    "\n",
    "Specifically, letting $k(x,y)=\\langle \\phi(x), \\phi(y)\\rangle$, you get\n",
    "$$\\text{MMD}(P,Q)=\\mathbb{E}_{X, X' \\sim P}k(x, x')+\\mathbb{E}_{Y, Y'\\sim Q}k(Y, Y')\n",
    "-2\\mathbb{E}_{X\\sim P, Y\\sim Q}k(X,Y)$$\n",
    "--> can straightforwardly estimate with samples.\n",
    "\n",
    "\n",
    "An alternative characterization of the MMD:\n",
    "$$\\text{MMD}(P,Q)=\\sup_{f\\in H:\\|f\\|_H\\leq 1}\\|\\mathbb{E}_{X\\sim P}[f(X)]-\\mathbb{E}_{Y\\sim Q}[f(Y)]\\|.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually, the implementation is MKMMD as shown in DAN and the kernel_num means the number of guassian kernel. I consider the kernel_mul is a parameter to control the bandwidth. By 'bandwidth /= kernel_mul ** (kernel_num // 2)', we could get the min_bandwidth. Then, you need compute 5(kernel_num) bandwidth for each kernel. You get bandwidth_list = [min_bandwidth, min_bandwidth * kernel_mul, min_bandwidth * kernel_mul^2 ..... ]\n",
    "\n",
    "It is ok to use one kernel, but MKMMD could achieve better performance as DAN shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class MMD_loss(nn.Module):\n",
    "    def __init__(self, kernel_mul = 2.0, kernel_num = 5):\n",
    "        super(MMD_loss, self).__init__()\n",
    "        self.kernel_num = kernel_num\n",
    "        self.kernel_mul = kernel_mul\n",
    "        self.fix_sigma = None\n",
    "\n",
    "    def guassian_kernel(self, source, target, kernel_mul=2.0, kernel_num=5, fix_sigma=None):\n",
    "        n_samples = int(source.size()[0])+int(target.size()[0])\n",
    "        total = torch.cat([source, target], dim=0)\n",
    "\n",
    "        total0 = total.unsqueeze(0).expand(int(total.size(0)), int(total.size(0)), int(total.size(1)))\n",
    "        total1 = total.unsqueeze(1).expand(int(total.size(0)), int(total.size(0)), int(total.size(1)))\n",
    "        L2_distance = ((total0-total1)**2).sum(2) \n",
    "        if fix_sigma:\n",
    "            bandwidth = fix_sigma\n",
    "        else:\n",
    "            bandwidth = torch.sum(L2_distance.data) / (n_samples**2-n_samples)\n",
    "        bandwidth /= kernel_mul ** (kernel_num // 2)\n",
    "        bandwidth_list = [bandwidth * (kernel_mul**i) for i in range(kernel_num)]\n",
    "        kernel_val = [torch.exp(-L2_distance / bandwidth_temp) for bandwidth_temp in bandwidth_list]\n",
    "        return sum(kernel_val)\n",
    "    \n",
    "    def forward(self, source, target):\n",
    "        batch_size = int(source.size()[0])\n",
    "        kernels = guassian_kernel(source, target, kernel_mul=self.kernel_mul, kernel_num=self.kernel_num, fix_sigma=self.fix_sigma)\n",
    "        XX = kernels[:batch_size, :batch_size]\n",
    "        YY = kernels[batch_size:, batch_size:]\n",
    "        XY = kernels[:batch_size, batch_size:]\n",
    "        YX = kernels[batch_size:, :batch_size]\n",
    "        loss = torch.mean(XX + YY - XY -YX)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
