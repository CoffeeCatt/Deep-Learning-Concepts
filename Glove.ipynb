{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import requests\n",
    "import math\n",
    "import zipfile\n",
    "from contextlib import contextmanager\n",
    "from tempfile import TemporaryDirectory\n",
    "from tqdm import tqdm\n",
    "\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def maybe_download(url, filename=None, work_directory=\".\", expected_bytes=None):\n",
    "    \"\"\"Download a file if it is not already downloaded.\n",
    "    Args:\n",
    "        filename (str): File name.\n",
    "        work_directory (str): Working directory.\n",
    "        url (str): URL of the file to download.\n",
    "        expected_bytes (int): Expected file size in bytes.\n",
    "        \n",
    "    Returns:\n",
    "        str: File path of the file downloaded.\n",
    "    \"\"\"\n",
    "    if filename is None:\n",
    "        filename = url.split(\"/\")[-1]\n",
    "    os.makedirs(work_directory, exist_ok=True)\n",
    "    filepath = os.path.join(work_directory, filename)\n",
    "    if not os.path.exists(filepath):\n",
    "\n",
    "        r = requests.get(url, stream=True)\n",
    "        total_size = int(r.headers.get(\"content-length\", 0))\n",
    "        block_size = 1024\n",
    "        num_iterables = math.ceil(total_size / block_size)\n",
    "\n",
    "        with open(filepath, \"wb\") as file:\n",
    "            for data in tqdm(\n",
    "                r.iter_content(block_size),\n",
    "                total=num_iterables,\n",
    "                unit=\"KB\",\n",
    "                unit_scale=True,\n",
    "            ):\n",
    "                file.write(data)\n",
    "    else:\n",
    "        log.info(\"File {} already downloaded\".format(filepath))\n",
    "    if expected_bytes is not None:\n",
    "        statinfo = os.stat(filepath)\n",
    "        if statinfo.st_size != expected_bytes:\n",
    "            os.remove(filepath)\n",
    "            raise IOError(\"Failed to verify {}\".format(filepath))\n",
    "\n",
    "    return filepath\n",
    "\n",
    "\n",
    "\n",
    "def download_path(path=None):\n",
    "    \"\"\"Return a path to download data. If `path=None`, then it yields a temporal path that is eventually deleted, \n",
    "    otherwise the real path of the input. \n",
    "    Args:\n",
    "        path (str): Path to download data.\n",
    "    Returns:\n",
    "        str: Real path where the data is stored.\n",
    "    Examples:\n",
    "        >>> with download_path() as path:\n",
    "        >>> ... maybe_download(url=\"http://example.com/file.zip\", work_directory=path)\n",
    "    \"\"\"\n",
    "    if path is None:\n",
    "        tmp_dir = TemporaryDirectory()\n",
    "        try:\n",
    "            yield tmp_dir.name\n",
    "        finally:\n",
    "            tmp_dir.cleanup()\n",
    "    else:\n",
    "        path = os.path.realpath(path)\n",
    "        yield path\n",
    "\n",
    "\n",
    "def unzip_file(zip_src, dst_dir, clean_zip_file=True):\n",
    "    \"\"\"Unzip a file\n",
    "    Args:\n",
    "        zip_src (str): Zip file.\n",
    "        dst_dir (str): Destination folder.\n",
    "        clean_zip_file (bool): Whether or not to clean the zip file.\n",
    "    \"\"\"\n",
    "    fz = zipfile.ZipFile(zip_src, \"r\")\n",
    "    for file in fz.namelist():\n",
    "        fz.extract(file, dst_dir)\n",
    "    if clean_zip_file:\n",
    "        os.remove(zip_src)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_extract_globe(dest_path):\n",
    "    url = \"http://nlp.stanford.edu/data/glove.6B.zip\"\n",
    "    filepath = maybe_download(url=url, work_directory=dest_path)\n",
    "    glove_path = os.path.join(dest_path, \"glove\")\n",
    "    unzip_file(filepath, glove_path, clean_zip_file=False)\n",
    "    return glove_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_matrix(path_emb, word_dict, word_embedding_dim):\n",
    "    '''Load pretrained embedding metrics of words in word_dict\n",
    "    \n",
    "    Args: \n",
    "        path_emb (string): Folder path of downloaded glove file\n",
    "        word_dict (dict): word dictionary\n",
    "        word_embedding_dim: dimention of word embedding vectors\n",
    "        \n",
    "    Returns:\n",
    "        numpy array, list: pretrained word embedding metrics, words can be found in glove files\n",
    "    '''\n",
    "    \n",
    "    embedding_matrix = np.zeros((len(word_dict)+1, word_embedding_dim))\n",
    "    exist_word=[]\n",
    "\n",
    "    with open(os.path.join(path_emb, f\"glove.6B.{word_embedding_dim}d.txt\"),'rb') as f:\n",
    "        for l in tqdm(f):\n",
    "            l=l.split()\n",
    "            word = l[0].decode()\n",
    "            if len(word) != 0:\n",
    "                if word in word_dict:\n",
    "                    wordvec = [float(x) for x in l[1:]]\n",
    "                    index = word_dict[word]\n",
    "                    embedding_matrix[index]=np.array(wordvec)\n",
    "                    exist_word.append(word)\n",
    "                    \n",
    "    return embedding_matrix, exist_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_zip, valid_zip = download_mind(size=mind_type, dest_path=data_path)\n",
    "unzip_file(train_zip, os.path.join(data_path, 'train'))\n",
    "unzip_file(valid_zip, os.path.join(data_path, 'valid'))\n",
    "output_path = os.path.join(data_path, 'utils')\n",
    "os.makedirs(output_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_cnt = Counter()\n",
    "word_cnt_all = Counter()\n",
    "news = pd.read_table(os.path.join(data_path, 'train', 'news.tsv'),\n",
    "                     names=['newid', 'vertical', 'subvertical', 'title',\n",
    "                            'abstract', 'url', 'entities in title', 'entities in abstract'],\n",
    "                     usecols = ['vertical', 'subvertical', 'title', 'abstract'])\n",
    "\n",
    "for i in tqdm(range(len(news))):\n",
    "    word_cnt.update(news.loc[i]['title'])\n",
    "    \n",
    "word_dict = {k: v+1 for k, v in zip(word_cnt, range(len(word_cnt)))}\n",
    "\n",
    "tmpdir = TemporaryDirectory()\n",
    "data_path = tmpdir.name\n",
    "glove_path = download_and_extract_globe(data_path)\n",
    "embedding_matrix, exist_word = load_glove_matrix(glove_path, word_dict, word_embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(os.path.join(output_path, 'embedding.npy'), embedding_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
