{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/perplexity-in-language-models-87a196019a94"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can in fact use two different approaches to evaluate and compare language models:\n",
    "\n",
    "Extrinsic evaluation: employ them in an actual task (such as machine translation) and looking at their final loss/accuracy. \n",
    "Computationally expensive and slow as it requires training a full system.\n",
    "\n",
    "Intrinsic evaluation: find some metric to evaluate the language model itself.\n",
    "not as “good” as extrinsic evaluation, it’s a useful way of quickly comparing models. \n",
    "Perplexity is an intrinsic evaluation method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perplexity as the normalized inverse probability of the test set\n",
    "\n",
    "$$PPL (W) =  \\sqrt[N]{\\frac{1}{P(w_1, w_2, \\cdots, w_N)}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lower PPL (the better) means the model assigns a higher probability to the test examples.\n",
    "\n",
    "the N-th root is used to normalized the probability by the size of test size\n",
    "\n",
    "--> consider the unigram\n",
    "$$ \\log PPL(W) = - \\frac{1}{N}[\\log P(w_1)\\cdots \\log P(w_N)]$$\n",
    "\n",
    "It’s basically calculating the exponential of cross-entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation in pytorch-seq2seq\n",
    "# https://github.com/IBM/pytorch-seq2seq/blob/f146087a9a271e9b50f46561e090324764b081fb/seq2seq/loss/loss.py\n",
    "def get_loss(self):\n",
    "    nll = super(Perplexity, self).get_loss()\n",
    "    nll /= self.norm_term.item()\n",
    "    if nll > Perplexity._MAX_EXP:\n",
    "        print(\"WARNING: Loss exceeded maximum value, capping to e^100\")\n",
    "        return math.exp(Perplexity._MAX_EXP)\n",
    "    return math.exp(nll)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/54941966/how-can-i-calculate-perplexity-using-nltk/55043954"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLE Estimates: [(('an', ()), 0.5), (('apple', ()), 0.25)]\n",
      "MLE Estimates: [(('an', ()), 0.5), (('ant', ()), 0.0)]\n",
      "PP(an apple):2.8284271247461903\n",
      "PP(an ant):inf\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from nltk.lm import MLE\n",
    "\n",
    "train_sentences = ['an apple', 'an orange']\n",
    "tokenized_text = [list(map(str.lower, nltk.tokenize.word_tokenize(sent))) \n",
    "                for sent in train_sentences]\n",
    "n = 1\n",
    "train_data, padded_vocab = padded_everygram_pipeline(n, tokenized_text)\n",
    "model = MLE(n)\n",
    "model.fit(train_data, padded_vocab)\n",
    "\n",
    "test_sentences = ['an apple', 'an ant']\n",
    "tokenized_text = [list(map(str.lower, nltk.tokenize.word_tokenize(sent))) \n",
    "                for sent in test_sentences]\n",
    "\n",
    "test_data, _ = padded_everygram_pipeline(n, tokenized_text)\n",
    "for test in test_data:\n",
    "    print (\"MLE Estimates:\", [((ngram[-1], ngram[:-1]),model.score(ngram[-1], ngram[:-1])) for ngram in test])\n",
    "\n",
    "test_data, _ = padded_everygram_pipeline(n, tokenized_text)\n",
    "\n",
    "for i, test in enumerate(test_data):\n",
    "    print(\"PP({0}):{1}\".format(test_sentences[i], model.perplexity(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLE Estimates: [(('an', ('<s>',)), 1.0), (('apple', ('an',)), 0.5), (('</s>', ('apple',)), 1.0)]\n",
      "MLE Estimates: [(('an', ('<s>',)), 1.0), (('ant', ('an',)), 0.0), (('</s>', ('ant',)), 0)]\n",
      "PP(an apple):1.2599210498948732\n",
      "PP(an ant):inf\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from nltk.lm import MLE\n",
    "from nltk.lm import Vocabulary\n",
    "\n",
    "train_sentences = ['an apple', 'an orange']\n",
    "tokenized_text = [list(map(str.lower, nltk.tokenize.word_tokenize(sent))) for sent in train_sentences]\n",
    "\n",
    "n = 2\n",
    "train_data = [nltk.bigrams(t,  pad_right=True, pad_left=True, left_pad_symbol=\"<s>\", right_pad_symbol=\"</s>\") for t in tokenized_text]\n",
    "words = [word for sent in tokenized_text for word in sent]\n",
    "words.extend([\"<s>\", \"</s>\"])\n",
    "padded_vocab = Vocabulary(words)\n",
    "model = MLE(n)\n",
    "model.fit(train_data, padded_vocab)\n",
    "\n",
    "test_sentences = ['an apple', 'an ant']\n",
    "tokenized_text = [list(map(str.lower, nltk.tokenize.word_tokenize(sent))) for sent in test_sentences]\n",
    "\n",
    "test_data = [nltk.bigrams(t,  pad_right=True, pad_left=True, left_pad_symbol=\"<s>\", right_pad_symbol=\"</s>\") for t in tokenized_text]\n",
    "for test in test_data:\n",
    "    print (\"MLE Estimates:\", [((ngram[-1], ngram[:-1]),model.score(ngram[-1], ngram[:-1])) for ngram in test])\n",
    "\n",
    "test_data = [nltk.bigrams(t,  pad_right=True, pad_left=True, left_pad_symbol=\"<s>\", right_pad_symbol=\"</s>\") for t in tokenized_text]\n",
    "for i, test in enumerate(test_data):\n",
    "    print(\"PP({0}):{1}\".format(test_sentences[i], model.perplexity(test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
