{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The statistics are calculated across the feature dimension, for each element and instance independently (source). \n",
    "In NLP tasks, the sentence length often varies -- thus, if using batchnorm, it would be uncertain what would be the appropriate normalization constant (the total number of elements to divide by during normalization) to use. \n",
    "\n",
    "Different batches would have different normalization constants which leads to instability during the course of training. According to the paper that provided the image linked above, \"statistics of NLP data across the batch dimension exhibit large fluctuations throughout training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x: (bs, c, feature_dim)\n",
    "mean = x.mean(dim=-1, keepdim=True)\n",
    "var = ((x - mean) ** 2).mean(dim=-1, keepdim=True)\n",
    "std = (var + epsilon).sqrt()\n",
    "y = (x - mean) / std"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
